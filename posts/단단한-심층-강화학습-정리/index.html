<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.3.1" /><meta property="og:title" content="단단한 심층 강화학습 정리" /><meta property="og:locale" content="en" /><meta name="description" content="단단한 심층 강화학습" /><meta property="og:description" content="단단한 심층 강화학습" /><link rel="canonical" href="https://dgg1dbg.github.io/posts/%EB%8B%A8%EB%8B%A8%ED%95%9C-%EC%8B%AC%EC%B8%B5-%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5-%EC%A0%95%EB%A6%AC/" /><meta property="og:url" content="https://dgg1dbg.github.io/posts/%EB%8B%A8%EB%8B%A8%ED%95%9C-%EC%8B%AC%EC%B8%B5-%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5-%EC%A0%95%EB%A6%AC/" /><meta property="og:site_name" content="Jeong Min’s blog" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2024-07-17T03:00:00+00:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="단단한 심층 강화학습 정리" /><meta name="twitter:site" content="@x" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-07-17T03:00:00+00:00","datePublished":"2024-07-17T03:00:00+00:00","description":"단단한 심층 강화학습","headline":"단단한 심층 강화학습 정리","mainEntityOfPage":{"@type":"WebPage","@id":"https://dgg1dbg.github.io/posts/%EB%8B%A8%EB%8B%A8%ED%95%9C-%EC%8B%AC%EC%B8%B5-%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5-%EC%A0%95%EB%A6%AC/"},"url":"https://dgg1dbg.github.io/posts/%EB%8B%A8%EB%8B%A8%ED%95%9C-%EC%8B%AC%EC%B8%B5-%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5-%EC%A0%95%EB%A6%AC/"}</script><title>단단한 심층 강화학습 정리 | Jeong Min's blog</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Jeong Min's blog"><meta name="application-name" content="Jeong Min's blog"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get MODE_ATTR() { return "data-mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" class="mx-auto"> <img src="/assets/img/favicons/android-chrome-512x512.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title"> <a href="/">Jeong Min's blog</a></div><div class="site-subtitle font-italic">Coding, Math, Security</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <button class="mode-toggle btn" aria-label="Switch Mode"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/dgg1dbg" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/x" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['jeongmin.lihi','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper"><div id="topbar" class="container d-flex align-items-center justify-content-between h-100 pl-3 pr-3 pl-md-4 pr-md-4"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>단단한 심층 강화학습 정리</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper" class="d-flex justify-content-center"><div id="main" class="container pl-xl-4 pr-xl-4"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-9 pr-xl-4"><div class="post pl-1 pr-1 pl-md-2 pr-md-2"><h1 data-toc-skip>단단한 심층 강화학습 정리</h1><div class="post-meta text-muted"> <span> Posted <em class="" data-ts="1721185200" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Jul 17, 2024 </em> </span><div class="d-flex justify-content-between"> <span> By <em> <a href="https://twitter.com/username">Jeong Min Lee</a> </em> </span><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="6404 words"> <em>35 min</em> read</span></div></div></div><div class="post-content"><h1 id="단단한-심층-강화학습">단단한 심층 강화학습</h1><p><a href="../../assets/img/1/XL.jpeg" class="popup img-link "><img data-src="../../assets/img/1/XL.jpeg" alt="bookcover" class="lazyload" data-proofer-ignore></a></p><h1 id="0-소주제들">0. 소주제들</h1><h3 id="1-mdp와-pomdp"><span class="mr-2">1. MDP와 POMDP</span><a href="#1-mdp와-pomdp" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>실제 세계에서 거의 대부분의 문제들은 POMDP이다.</p><h1 id="1-강화학습-소개">1. 강화학습 소개</h1><h2 id="11-mdp로서의-강화학습"><span class="mr-2">1.1 MDP로서의 강화학습</span><a href="#11-mdp로서의-강화학습" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><ul><li>$s_t \in S$: 상태의 집합<li>$a_t \in A$: 행동의 집합<li>$P(s_{t+1}|s_t,a_t)$: 상태 전이 함수 ← Markov property<li>$R(s_t, a_t, s_{t+1}) = r_t$: 보상 함수<li>경로: $\tau = (s_0, a_0, r_0) , … \ ,(s_T, a_T, r_T)$<li>경로 보상 함수: $R(\tau)=r_0 + \gamma r_1 + \cdot \cdot \cdot + \gamma ^Tr_T = \sum_{t=0}^{T}\gamma ^t r_t$<li>목적함수: \(J(\tau) = \mathbb{E}_{\tau \sim \pi}[R(\tau)] = \mathbb{E}_{\tau }[\sum_{t=0}^{T}\gamma ^t r_t]\)</ul><p>→ 목적함수를 최대화 == 최대 보상을 받는 경로를 구성하는 것</p><h2 id="12-심층-강화학습-알고리즘의-종류"><span class="mr-2">1.2 심층 강화학습 알고리즘의 종류</span><a href="#12-심층-강화학습-알고리즘의-종류" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><ol><li><p>정책 기반</p><p>$J(\tau) = \mathbb{E}_{\tau \sim \pi}[R(\tau)]$ 최대화하는 $\pi$찾기</p><ul><li>최적 정책 수렴이 보장<li>분산이 크고 표본 비효율적</ul><p>ex: Reinforce</p><li><p>가치 기반</p>\[V^\pi(s)=\mathbb{E}_{s_0=s, \tau \sim \pi}[\sum_{t=0}^{T}\gamma_t r_t]\] \[Q^\pi(s,a)= \mathbb{E}_{s_0=s, a_0=a, \tau \sim \pi}[\sum_{t=0}^{T}\gamma_t r_t]\]<ul><li>표본 효율적<li>최적정책 수렴보장 X</ul><p>ex: Sarsa, Dqn</p><li><p>모델 기반</p><p>\(P(s_{t+1}\|s_t,a_t)\) 가 주어지거나, 학습</p><p>⇒ 최적 길찾기 문제</p><ul><li>모델 학습 어려움<li>오차 비약적으로 증가</ul><p>ex: mcts</p><li>on-policy: 정책에 대해서 학습 → 훈련과정에서 $\pi$로부터 생성된 데이터만을 이용<li>off-policy: 모든 데이터로 훈련 ⇒ data efficient</ol><h2 id="13-강화학습과-지도학습의-차이"><span class="mr-2">1.3 강화학습과 지도학습의 차이</span><a href="#13-강화학습과-지도학습의-차이" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><ul><li>오라클의 부재: 정확한 목표값을 모름<li>피드백의 희소성: 대부분 상태의 보상이 0에 수렴함<li>에이전트와 환경이 상호작용하여 데이터 생성: 오차가 커짐</ul><h1 id="2-reinforce">2. Reinforce</h1><p>이름에서 알 수 있듯이 좋은 결과를 초래했던 행동을 강화(더 높은 확률로 선택되도록)하는 것이다.</p><p>⇒ \(J(\pi_\theta) = \mathbb{E}_{\tau \sim \pi_\theta}[R(\tau)] = \mathbb{E} _{\tau \sim \pi_\theta}[\sum_{t=0}^{T}\gamma ^t r_t]\)를 최대화</p><p>⇒ \(\theta \leftarrow \theta + \alpha \nabla_\theta J(\pi_\theta)\), \(\nabla_\theta J(\pi_\theta) = \mathbb{E}_{\tau \sim \pi_\theta}[\sum_{t=0}^T R_t(\tau)\nabla_\theta \log\pi_\theta(a_t\|s_t)]\)</p><h2 id="21-정책-경사-계산"><span class="mr-2">2.1 정책 경사 계산</span><a href="#21-정책-경사-계산" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>\(\nabla_\theta J(\pi_\theta) = \nabla_\theta\mathbb{E}_{\tau \sim \pi_\theta}[R(\tau)] = \mathbb{E}_{\tau \sim \pi_\theta}[\sum_{t=0}^T R_t(\tau)\nabla_\theta \log\pi_\theta(a_t\|s_t)]\)를 유도해보자.</p><p>보조정리 1.</p>\[\nabla_\theta\mathbb{E}_{x \sim p(x\|\theta)}[f(x)] = \mathbb{E}_{x \sim p(x\|\theta)}[f(x)\nabla_\theta\log p(x\|\theta)]\]<p>보조정리 2.</p>\[\log p(\tau \|\theta) = \sum_{t\geq0}(\log p(s_{t+1}\|s_t,a_t) + \log \pi_\theta(a_t\|s_t))\]<p>보조정리 1에 의해, \(\nabla_\theta\mathbb{E}_{\tau \sim \pi_\theta}[R(\tau)] = \mathbb{E}_{\tau \sim \pi_\theta}[R(\tau)\nabla_\theta\log p(\tau \|\theta)]\)가 성립하고, 보조정리 2에 의해, \(\mathbb{E}_{\tau \sim \pi_\theta}[R(\tau)\nabla_\theta\log p(\tau \|\theta)] = \mathbb{E}_{\tau\sim\pi_\theta}[\sum_{t=0}^TR(\tau)\nabla_\theta\log\pi_\theta(a_t \|s_t)]\)가 성립한다. 한편, $a_t$를 선택하는 행위는 시간 $t$ 이후로만 영향을 끼치며, 몬테카를로 법칙에 따라 다음이 성립하게 된다.</p>\[\nabla_\theta J(\pi_\theta) = \nabla_\theta\mathbb{E}_{\tau \sim \pi_\theta}[R(\tau)] \approx \sum_{t=0}^T R_t(\tau)\nabla_\theta \log\pi_\theta(a_t|s_t)\]<h2 id="22-알고리즘-구현"><span class="mr-2">2.2 알고리즘 구현</span><a href="#22-알고리즘-구현" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><ol><li>한 에피소드를 진행하고, 데이터를 모은다.<li>$R_t(\tau)$를 recursive하게 계산한다.<li>이를 활용하여 $\nabla_\theta J(\pi_\theta)$를 계산하고, 파라미터를 업데이트 한다.</ol><p><em>중요 포인트</em></p><ol><li>$R_t(\tau)$를 계산할 때, 종료시점 부터 역순으로 recursive하게 계산하면 $t$시간만에 계산이 가능하다.<li><p>$R_t(\tau)$에 특정 기준 값을 빼줌으로서 더 유용한 분포를 만들 수 있다.</p><p>보상값이 대부분 음인 경우 유용</p></ol><h1 id="3-sarsa">3. SARSA</h1><h2 id="31-q함수와-v함수"><span class="mr-2">3.1 Q함수와 V함수</span><a href="#31-q함수와-v함수" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>\[V^\pi(s)=\mathbb{E}_{s_0=s, \tau \sim \pi}[\sum_{t=0}^{T}\gamma_t r_t]\] \[Q^\pi(s,a)= \mathbb{E}_{s_0=s, a_0=a, \tau \sim \pi}[\sum_{t=0}^{T}\gamma_t r_t]\]<ol><li><p>Q함수는 에이전트에게 행동할 수 있는 직접적인 방법을 제공해 준다.</p><p>Q함수를 통해 에이전트는 상태 s에 대해 최대의 Q값을 갖는 a를 바로 선택할 수 있다.</p><p>반면 V함수는, s에서 선택 가능한 모든 a를 통해 다음 상태 s’들의 V값을 계산해서 행동해야 한다.</p><p>그러나 이는 비용이 크고, 전이함수를 모르면 쉽지 않다.</p><li><p>Q함수는 계산하기 더 복잡하고, 학습하기 위해 더 많은 데이터가 필요하다.</p><p>상태 s에서 나온 데이터는 상태 s의 V함수에 모두 활용될 수 있지만, Q함수에는 일치하는 행동에만 활용이 된다.</p></ol><h2 id="32-시간차-학습"><span class="mr-2">3.2 시간차 학습</span><a href="#32-시간차-학습" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Reinforce에서 사용된 것과 같은 몬테카를로 알고리즘은, 한 에피소드가 끝날 때까지 기다린 후 학습할 수 있다는 단점이 있다.</p><p>이를 해결하기 위해, SARSA에서는 TD학습을 활용한다. 벨만 방정식에 따라, Q함수를 recursive하게 정의할 수 있다.</p>\[Q^\pi(s,a)=\mathbb{E}_{s'\sim p(s' \|s,a), \ \ r\sim R(s,a,s')}[r+\gamma\mathbb{E}_{a'\sim \pi(s')}[Q^\pi(s',a')]]\]<p>이를 TD 학습을 적용하면, Q함수에 대한 추정값으로 바꿀 수 있다.</p><p>$Q^\pi(s,a)\approx r+\gamma Q^\pi(s’,a’)=Q_{tar}^\pi(s,a)$ 즉 현재 계산한 Q함수를 target Q함수에 가깝게 업데이트 하는 것이다. 지도학습과 유사하다.</p><p>위 식의 의미는, 한 단계 미래의 상태를 봄을 통해, 미래에서 일어난 사건을 점진적으로 현재까지 반영하여 Q함수를 계산하게 된다는 것이다.</p><p>이는 에이전트의 특적 목적은, 궤적이 진행됨에 따라 드러난다는 점에서 가치가 있다.</p><p>한편, target Q함수에서 두 번의 기댓값을 생략했다. 바깥쪽 기댓값은 궤적을 충분히 많이 가짐으로서 해결할 수 있다.</p><p>안쪽의 기댓값은 해결하는 방식에 따라 SARSA와 DQN으로 구분할 수 있다.</p><ol><li>SARSA: 실제로 다음 상태에서 취해진 행동으로 근사<li>DQN: 가장 큰 Q함수 값을 갖는 행동으로 근사</ol><p>또한 식의 파라미터 $\gamma$에 대해 생각해볼 필요가 있다. $\gamma$가 작으면, 에이전트는 근시안적이 되고 바로 일어날 보상에 대해서만 관심을 갖게 된다. 반면, $\gamma$가 크면, 에이전트는 목표 상태에 도달하는 속도에 큰 관심을 갖지 않게 될 것이다. 또한, $\gamma$이 작으면, Q함수의 학습속도는 빠르지만, 에이전트의 성능은 저하될 수 있다. 반대로 $\gamma$가 크면, 학습속도는 느리지만, 학습결과는 더 우수해진다.</p><h2 id="33-탐험과-활용"><span class="mr-2">3.3 탐험과 활용</span><a href="#33-탐험과-활용" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>local minimum에 빠지지 않도록, 탐험과 활용의 비율을 적절히 조절해야한다. 왜냐하면, 탐욕만을 추구할 경우, Q함수의 편향은 실제 행동의 편향으로 이어질 것이기 때문이다.</p><p>따라서, SARSA에서는 $\epsilon$-greedy policy를 사용한다.</p><h2 id="34-알고리즘-구현"><span class="mr-2">3.4 알고리즘 구현</span><a href="#34-알고리즘-구현" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><ol><li>현재 상태 $s$, 현재 행동 $a$를 통해 그다음 상태 $s’$를 도출한다.<li>$\epsilon$-greedy policy를 활용하여 $a’$를 찾는다.<li>target Q함수와 Q함수의 오차를 활용하여 파라미터를 업데이트 한다.<li>$s \leftarrow s’$, $a \leftarrow a’$로 업데이트 하고, 이를 반복한다.</ol><p>중요 포인트</p><ol><li><p>SARSA는 on-policy 알고리즘이므로 특정 정책에서 얻은 경험은 그 정책을 개선하는데만 사용될 수 있다.</p><p>target Q값은 현재의 정책으로 인해 greedy하게 선택된 행동을 통해 계산하였기 때문이다.</p><li><p>Reinforce와 달리 시간차 알고리즘을 통해 일정량의 데이터(배치)를 모은 후 train을 할 수 있다. → batch training(←→episodic training)</p></ol><h1 id="4-dqn">4. DQN</h1><h2 id="41-dqn의-q함수-학습"><span class="mr-2">4.1 DQN의 Q함수 학습</span><a href="#41-dqn의-q함수-학습" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>$Q_{tar:SARSA}^\pi(s,a) = r+\gamma Q^\pi(s’,a’)$</p><p>$Q_{tar:DQN}^\pi(s,a) = r+\gamma \max_{a’}Q^\pi(s’,a’)$</p><p>SARSA와 DQN의 타깃 Q함수는 위와 같은 차이가 있다. 다음 상태 s’에서 정책에 따라 <strong>실제 취해진</strong> 행동 a’에 대해 계산하지 않고, DQN은 선택가능한 모든 a’에 대한 Q함수중 가장 최대인 a’에 대해 계산한다.</p><p>즉 DQN은 데이터가 샘플링된 정책과 상관없이, 특정 s’에 대해 같은 Q함수 값을 도출한다. 이를 통해 DQN은 경험과 정책의 상관성에서 벗어날 수 있다.</p><p>또한 Q함수에 max를 취해준다는 것은, (s’,a’)에 대한 Q함수의 추정값이 정확하다는 가정 아래, Q함수를 최대화하는 행동을 선택하는 것이다. 이것이 에이전트가 할 수 있는 최선이다.</p><h2 id="42-dqn의-행동-선택"><span class="mr-2">4.2 DQN의 행동 선택</span><a href="#42-dqn의-행동-선택" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>DQN이 off-policy 알고리즘이지만, 여전히 경험을 수집하는 방식은 중요한 문제이다. 상태-행동 공간이 아주 클 경우, naive DQN은 모든 상태를 경험하기 힘들 것이다.</p><p>이는 Q함수를 신경망으로 근사함으로서 어느 정도는 해결 될 수 있지만, 불연속적인 보상에 대해서는 예측이 어려움 등 여전히 한계가 존재한다.</p><p>따라서, DQN에서도 SARSA처럼 $\epsilon$-greedy policy를 사용하거나, 볼츠만 정책을 활용함을 통해 문제를 해결할 수 있다.</p><h3 id="볼츠만-정책"><span class="mr-2">볼츠만 정책</span><a href="#볼츠만-정책" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>볼츠만 정책은 행동의 상대적인 Q가치를 이용하여 행동을 확률적으로 선택한다.</p>\[p_{softmax}(a \|s)=\frac{e^{Q^\pi(s,a)}}{\sum_{a'}e^{Q^\pi(s,a')}}\] \[p_{boltzman}(a \|s)=\frac{e^{Q^\pi(s,a)/\tau}}{\sum_{a'}e^{Q^\pi(s,a')/\tau}}\]<p>$\tau$를 크게 하면, 분포는 더 균일해지고, $\tau$를 작게하면 분포는 더 집중된 분포를 가진다.</p><p>$\epsilon$-greedy policy에 비교했을 때의 장점은, 환경을 덜 무작이로 탐험한다는 것이다. $\epsilon$의 확률로 무작위로 선택하는 것이 아닌, Q가치에 비례하여 무작위로 행동을 선택한다.</p><p>따라서 볼츠만 정책은 $\epsilon$-greedy policy보다 Q가치 추정값과 행동 확률 사이의 관계를 더 부드럽게 만든다.</p><p>다만 Q가치 추정값이 최적 Q가치와 오차가 큰 경우, $\epsilon$-greedy policy가 더 유리하다. 이를 해결하기 위해 훈련 초기에 $\tau$를 키울 수 있다.</p><h2 id="43-경험-재현"><span class="mr-2">4.3 경험 재현</span><a href="#43-경험-재현" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>On-policy 알고리즘은</p><ol><li>업데이트를 위해 오직 현재 정책에 따라 수집된 데이터만을 이용할 수 있다. 따라서 타깃 Q함수와 기존 Q함수의 차이가 큰 경우와 같이 한 경험으로 여러번의 파라미터 업데이트가 필요한 문제를 해결하지 못한다.<li>수집된 데이터들은 한 정책으로부터 나온 것이므로 서로 밀접하게 연관되어있다. 이로 인해 파라미터 업데이트의 분산이 커질 수 있다.</ol><p>반면 Off-policy 알고리즘은</p><ol><li>한번 사용된 경험을 폐기할 필요가 없다. ⇒ experience replay(경험 재현)<li>경험들의 연관성이 낮아진다.</ol><h2 id="44-알고리즘-구현"><span class="mr-2">4.4 알고리즘 구현</span><a href="#44-알고리즘-구현" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><ol><li>에피소드를 최소량의 경험 데이터가 모일때까지 진행한다.<li>이후 주기적으로, 경험 데이터를 통해 파라미터를 업데이트 한다.</ol><p>중요 포인트</p><ol><li>expierence replay를 통해 batch방식으로 파라미터를 업데이트할 수 있다.</ol><h1 id="5-향상된-dqn">5. 향상된 DQN</h1><h2 id="51-목표-네트워크"><span class="mr-2">5.1 목표 네트워크</span><a href="#51-목표-네트워크" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>DQN은 TD학습을 이용한다. 즉, 현재 Q함수를 타깃 Q함수의 방향으로 파라미터를 업데이트 한다.</p><p>여기서 중요한 점은, 두 Q함수를 근사하는데 활용된 파라미터가 동일하다는 점이다.</p><p>이로 인해 발생하는 문제점은, 업데이트의 목표도 계속 함께 움직인다는 것이다.</p><p>이를 해결하기 위해, 타깃 Q함수를 위한 별도의 네트워크를 구성하고, 이 목표 네트워크를 주기적으로 메인 네트워크로 업데이트 한다.</p><p>이를 통해 일반적인 지도 회귀 문제로 전환할 수 있다.</p><p>이를 치환 업데이트 라고 부르며, 이와 다르게 폴리악(polyak) 업데이트를 시행할 수도 있다.</p><p>목표 네트워크의 추가로 인해 훈련속도가 저하될 수 있다는 단점이 있다. 이는 업데이트 주기를 조절함을 통해 해결할 수 있다.</p><h2 id="52-이중-dqn"><span class="mr-2">5.2 이중 DQN</span><a href="#52-이중-dqn" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>DQN은 타깃 함수를 추정할 때, 추정값의 최댓값을 활용한다. 그러나, 타깃함수에 오차가 조금이라도 있다면, 최댓값이 양의 방향으로 편향되어 과대 추정이 일어나게 된다.</p><p>$Q_{tar:DQN}^\pi(s,a) = r+\gamma \max_{a’}Q^\pi(s’,a’)$에서 max를 취함으로서 발생하는 문제이다.</p><p>이것은 최대 기댓값 과대 추정이라는 문제인데, $s’$에서 취할 수 있는 $a’$가 많을 수록, 더욱 크게 편향된다.</p><p>한편, DQN은 자주 경험한 쌍에 대해 더 자주 과대 추정을 하게 되고, 에이전트가 균일하게 탐험하지 못했을 경우, 이는 악순환을 만든다.</p><p>아래 코드와 그래프로 최대 기댓값 추정 문제를 시뮬레이션 해볼 수 있다. 비록 정확한 기댓값을 계산하지는 않았지만(아래에서는 mean으로 계산), 결과와 의미 자체는 동일하다.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="k">def</span> <span class="nf">maxexpred</span><span class="p">(</span><span class="n">n_act</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>
        <span class="n">s</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_act</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">30</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="n">maxexpred</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">x</span><span class="p">]</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">savefig</span><span class="p">(</span><span class="s">'img.png'</span><span class="p">)</span>
</pre></table></code></div></div><p><a href="../../assets/img/1/Untitled.png" class="popup img-link "><img data-src="../../assets/img/1/Untitled.png" alt="실험 결과" class="lazyload" data-proofer-ignore></a></p><p>아무튼 이러한 문제점을 해결하는 방법은, 다음과 같이 네트워크를 하나 추가하는 것이다.</p><p>$Q_{tar:DoubleDQN}^\pi(s,a) = r + \gamma Q^{\pi_\varphi}(s’,\max_{a’}Q^{\pi_\theta}(s’,a’))$</p><p>그리고 이를 구현할 시에는, 위에서 살펴본 목표 네트워크로 두 가지 역할을 모두 수행하면 된다.</p><h2 id="53-우선순위가-있는-경험-재현per"><span class="mr-2">5.3 우선순위가 있는 경험 재현(PER)</span><a href="#53-우선순위가-있는-경험-재현per" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>직관적으로, 새로운 작업을 할 때, 어떤 경험이 다른 것보다 더 많은 것을 알려주는 상황을 생각해볼 수 있다.</p><p>DQN에서는 TD오차가 큰 경우가 해당될 것이다. 이러한 관점에서, 경험을 기억에서 무작위로 추출하기 보다는, 경험에 우선순위를 부여해서 추출하자는 개념이다.</p><p>\(P(i) = \frac{(\|w_i \| +\epsilon)^\eta}{\sum_j(\| w_j \| +\epsilon)^\eta}\), $w_i$는 i번째 경험에 대한 TD오차, $\eta$가 클 수록 우선순위를 강하게 부여</p><p>한편, 이와 같은 방식으로 경험의 분포에 대한 기댓값이 바뀌게 되고, 편차가 생기게 된다. 이를 해결하기 위해 TD오차에 가중치를 곱함으로써 보정할 수 있다.</p><p>이 방법을 importance sampling이라고 한다.</p><p><a href="https://hiddenbeginner.github.io/rl/2022/10/02/importance_sampling.html">[강화학습] Importance sampling이란?</a></p><h2 id="54-알고리즘-구현"><span class="mr-2">5.4 알고리즘 구현</span><a href="#54-알고리즘-구현" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>중요 포인트</p><ol><li>네트워크를 하나 추가함을 통해, target network와 double DQN을 동시에 구현할 수 있다.<li>PER을 구현할 때, 파라미터를 업데이트 할 때마다 경험들에 대한 TD오차를 다시 계산해야 한다.<li>Sum Tree를 사용하면, PER을 효율적으로 구현할 수 있다.</ol><h1 id="6-a2c">6. A2C</h1><h3 id="학습된-강화-신호가-환경에서-얻는-보상보다-정책에-대해-더-많은-것을-알려줄-수-있다"><span class="mr-2">학습된 강화 신호가 환경에서 얻는 보상보다 정책에 대해 더 많은 것을 알려줄 수 있다.</span><a href="#학습된-강화-신호가-환경에서-얻는-보상보다-정책에-대해-더-많은-것을-알려줄-수-있다" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><h2 id="61-행동자"><span class="mr-2">6.1 행동자</span><a href="#61-행동자" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>행동자는 정책 경사를 이용하여 정책 $\pi_\theta$를 학</p><p>습한다.</p><ul><li>A2C: \(\nabla _\theta J(\pi_\theta) = \mathbb{E}_t[A_t^\pi\nabla_\theta \log\pi_\theta(a_t \|s_t)]\)<li>Reinforce: \(\nabla _\theta J(\pi_\theta) = \mathbb{E}_t[R_t(\tau)\nabla_\theta \log\pi_\theta(a_t \|s_t)]\)</ul><h2 id="62-비평자"><span class="mr-2">6.2 비평자</span><a href="#62-비평자" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>비평자는 (s,a)쌍을 평가하는 방법을 학습하고, 그 결과를 이용하여 $A^\pi$를 생성한다.</p><h3 id="어드밴티지-함수"><span class="mr-2">어드밴티지 함수</span><a href="#어드밴티지-함수" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>어드밴티지 함수: $A^\pi(s_t,a_t)=Q^\pi(s_t,a_t)-V^\pi(s_t)$</p><ol><li><p>강화 신호의 품질이 우수하다.</p><p>상태 s에서 모든 행동을 고려하여 특정 행동 a를 평가하며 행동 a의 상대적 우수성을 강화 신호로 만든다.</p><p>강화 신호가 상대적이지 않다면, $Q^\pi(s_t,a_t) &gt;0$ 이고 $A^\pi(s_t,a_t)&lt;0$인 경우 문제가 생기게 된다.</p><li><p>행동의 장기적 효과를 포착한다.</p><p>어드밴티지는 정책이 현재 특별히 좋지 않은 상태에 있다고 해서 행동을 억제 하지 않는다.</p><p>반대로, 좋은 상태에 있는 정책에 대해 행동에 가점을 주지 않는다.</p><p>이를 통해 행동이 미래의 가치를 어떻게 변화시킬지만을 고려하여 행동을 평가하게 된다.</p></ol><p>어드밴티지를 추정하는 방법은 크게 두 가지가 있다.</p><p>두 방법 모두 V함수를 먼저 학습하고, V를 통해 Q를 계산한다.</p><p>이유는, Q함수가 V함수에 비해 더 복잡하고 더 많은 표본을 필요로 하며, Q로부터 V를 추정하는 것이 더 많은 계산을 필요로 하기 때문이다.</p><h3 id="1-n단계-이득n-step-forward-return"><span class="mr-2">1. n단계 이득(n-step forward return)</span><a href="#1-n단계-이득n-step-forward-return" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>n단계 이득에서 학습된 V로 Q를 추정하는 방법은 아래와 같다.</p>\[Q^\pi(s_t,a_t)=\mathbb{E}_{\tau\sim\pi}[r_t+\gamma r_{t+1} + \cdot\cdot\cdot+\gamma^nr_{t+n}]+\gamma^{n+1}V^\pi(s_{t+n+1})\] \[\approx r_t +\gamma r_{t+1} + \cdot\cdot\cdot+\gamma^n r_{t+n} + \gamma^{n+1}\hat{V}^\pi(s_{t+n+1})\]<p>등호는 보상의 합의 기댓값과 완벽한 V함수를 알고 있을 때 성립한다.</p><p>이를 계산 가능하도록, 기댓값 대신, 보상의 궤적을 사용하고, 비평자가 학습한 V함수를 사용한다.</p><p>n을 키우면 편향은 줄어들지만 분산은 커지고, n을 줄이면 분산은 줄어들지만 편향은 커진다.</p><p>$A^\pi_{NSTEP}(s_t,a_t)=Q^\pi(s_t,a_t)-V^\pi(s_t)$</p>\[\approx r_t +\gamma r_{t+1} + \cdot\cdot\cdot+\gamma^n r_{t+n} + \gamma^{n+1}\hat{V}^\pi(s_{t+n+1}) - \hat{V}^\pi(s_t)\]<h3 id="2-일반화된-어드밴티지-추정gae"><span class="mr-2">2. 일반화된 어드밴티지 추정(GAE)</span><a href="#2-일반화된-어드밴티지-추정gae" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>이 방법은 하나의 n값을 선택하는 문제를 해결하기 위해, 여러개의 n값을 이용한다.</p><p>이를 위해, n단계 어드밴티지 추정기를 다음과 같이 정의한다.</p><p>$A_t^\pi(n) = r_t + \gamma r_{t+1} + \cdot\cdot\cdot + \gamma^nV^\pi(s_{t+n})-V^\pi(s_t)$</p><p>그리고 GAE는 다음과 같이 어드밴티지 추정기의 지수가중평균으로 계산된다.</p><p>$A_{GAE}^\pi(s_t,a_t)=(1-\lambda)(A_t^\pi(1) + \lambda A_t^\pi(2) + \cdot\cdot\cdot)$</p><p>$\delta_t = r_t+\gamma V^\pi(s_{t+1})-V^\pi(s_t)$를 활용하면, 아래와 같은 공식으로 정리할 수 있다.</p>\[\begin{align} &amp; A^{\pi}_t(1) = \delta_t &amp; &amp;= r_t + \gamma V(s_{t+1}) - V(s_t) \\&amp; A^{\pi}_t(2) = \delta_t + \gamma \delta_{t+1} &amp; &amp;= r_t + \gamma r_{t+1} + \gamma^2 V(s_{t+2}) - V(s_t) \\&amp; A^{\pi}_t(3) = \delta_t + \gamma \delta_{t+1} + \gamma ^2 \delta_{t+2} &amp; &amp;= r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \gamma^3 V(s_{t+3}) - V(s_t)\end{align}\] \[\begin{align}A_{GAE}^{\pi}(s_t,a_t) &amp;= (1-\lambda)\Big(A_{t}^{\pi}(1) + \lambda A_{t}^{\pi}(2) + \lambda^2 A_{t}^{\pi}(3) + \cdots \Big) \\&amp;= (1-\lambda)\Big(\delta_t + \lambda(\delta_t + \gamma \delta_{t+1}) + \lambda^2(\delta_t + \gamma \delta_{t+1} + \gamma^2 \delta_{t+2})+ \cdots \Big) \\&amp;= (1-\lambda)\Big( \delta_t(1+\lambda+\lambda^2+\cdots) + \gamma\delta_{t+1}(\lambda+\lambda^2+\cdots) + \cdots \Big) \\&amp;= (1-\lambda)\left(\delta_t \frac{1}{1-\lambda} + \gamma \delta_{t+1}\frac{\lambda}{1-\lambda} + \cdots\right) \\&amp;= \sum_{l=0}^\infty (\gamma \lambda)^l \delta_{t+l}\end{align}\]<h3 id="3-어드밴티지-함수에-대한-학습"><span class="mr-2">3. 어드밴티지 함수에 대한 학습</span><a href="#3-어드밴티지-함수에-대한-학습" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>n단계 이득과 일반화된 어드밴티지 추정 모두, V함수에 대한 학습을 필요로 한다. 이는 DQN에서 했던것 과 같이 TD학습을 통해 타깃 함수와의 오차를 이용해 학습된다.</p><p>그리고 계산을 간단히 하기 위해, 각 알고리즘에서 어드밴티지 함수를 구성하는 방식을 활용해서 타깃함수를 만든다.</p><p>n-step: $V_{tar}^\pi(s_t)=r_t+\gamma r_{t+1} + \cdot\cdot\cdot + \gamma^n r_{t+n} + \gamma r^{r+1}\hat{V}^\pi(s_{t+n+1})$</p><p>GAE: $V_{tar}^\pi(s_t)=A^\pi_{GAE}(s_t,a_t)+\hat{V}^\pi(s_t)$</p><h2 id="63-알고리즘-구현"><span class="mr-2">6.3 알고리즘 구현</span><a href="#63-알고리즘-구현" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><ol><li>한 에피소드를 진행하고, 데이터를 모은다.<li>비평자 네트워크를 통해, V함수의 예측값을 계산한다.<li>V함수의 예측값을 활용하여 어드밴티지 함수를 계산한다.<li>어드밴티지 함수를 활용하여 타깃함수를 만든다.<li>행동자 네트워크를 활용하여 엔트로피를 계산한다.<li>비평자 네트워크의 손실값을 계산한다.<li>행동자 네트워크의 손실값을 계산한다.<li>파라미터를 업데이트한다.</ol><p>중요 포인트</p><ol><li>n-step이냐 GAE냐에 따라서 타깃함수를 만드는 법이 달라진다.<li>엔트로피값을 행동자 네트워크의 손실값에 빼줌을 통해 정책의 분포의 균일함을 유지할 수 있다.<li>비평자와 행동자 네트워크를 한개의 네트워크로 구성할 수 있다. 스칼라 가중치를 이용해 균형을 맞출 필요가 있다.</ol><h1 id="7-ppo">7. PPO</h1><h2 id="71-성능-붕괴"><span class="mr-2">7.1 성능 붕괴</span><a href="#71-성능-붕괴" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>성능 붕괴 문제란 정책 경사 알고리즘(Policy Gradient)에서 발생하는 문제이다. 파라미터 업데이트를 위한 이상적인 학습률을 결정하기 어렵기 때문에 발생한다.</p><p>적절하지 않은 학습률 $\alpha$는 local maxima에 빠트리거나, 좋은 정책을 건너뛰게 만들어 성능 붕괴를 일으킨다.</p><p>학습률을 결정하기 어려운 결정적인 원인은, 정책공간과 파라미터 공간이 정확하게 대응되지 않는다는 데 있다.</p><p>즉, \(d_\theta(\theta_1,\theta_2)=d_\theta(\theta_2,\theta_3)\nLeftrightarrow d_\pi(\pi_{\theta_1},\pi_{\theta_2})=d_\pi(\pi_{\theta_2},\pi_{\theta_3})\)라는 것이다.</p><p>이를 해결하기 위해, 파라미터로 업데이트로 새로 만든 정책의 상대적인 성능을 측정하는 방법이 필요하다.</p><h2 id="72-trpo"><span class="mr-2">7.2 TRPO</span><a href="#72-trpo" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>상대적 성능 식별자는 두 정책의 목적 함수 사이의 차이로 정의할 수 있다. 그리고 아래 식을 통해 계산 가능하다.</p>\[J(\pi')-J(\pi)=\mathbb{E}_{\tau \sim \pi'}[\sum_{t=0}^T\gamma^tA^\pi(s_t,a_t)]\]<p>증명은 아래와 같다.</p>\[\mathbb{E}_{\tau\sim\pi'}[\sum_{t=0}^T\gamma^tA^\pi(s_t,a_t)] = \mathbb{E}_{\tau\sim\pi'}[\sum_{t\ge0}\gamma^t(\mathbb{E}_{s_{t+1},r_t\sim p(s_{t+1},r_t|s_t,a_t)}[r_t+\gamma V^\pi(s_{t+1})] - V^\pi(s_t))]\]<p>어드밴티지 함수를 전개한다.</p>\[=\mathbb{E}_{\tau\sim\pi'}[\sum_{t\ge0}\gamma^t(r_t+\gamma V^\pi(s_{t+1})-V^\pi(s_t))] = \mathbb{E}_{\tau\sim\pi'}[\sum_{t\ge0}\gamma^tr_t] +\mathbb{E}_{\tau\sim\pi'}[\sum_{t\ge0}\gamma^{t+1} V^\pi(s_{t+1})-\sum_{t\ge0}\gamma^t V^\pi(s_t))]\]<p>내부의 기댓값이 사라지는 이유는, 바깥쪽의 기댓값은 전이함수로부터 나온 상태와 보상의 기댓값을 포함하기 때문이다. 또한 기댓값의 선형성을 활용하였다.</p>\[=J(\pi')+\mathbb{E}_{\tau\sim\pi'}[\sum_{t\ge1}\gamma^{t} V^\pi(s_{t})-\sum_{t\ge0}\gamma^t V^\pi(s_t))]=J(\pi')-\mathbb{E}_{\tau\sim\pi'}[V^\pi(s_0)]\]<p>식을 단순히 변형하였다.</p>\[=J(\pi')-\mathbb{E}_{\tau\sim\pi'}[J(\pi)]=J(\pi')-J(\pi)\]<p>증명 끝.</p><hr /><p>한편, \(J(\pi')-J(\pi)=\mathbb{E}_{\tau \sim \pi'}[\sum_{t=0}^T\gamma^tA^\pi(s_t,a_t)]\) 이 식에는 문제점이 있다.</p><p>업데이트 이전에 새로운 정책으로 부터 추출된 궤적으로부터 기댓값을 계산해야한다는 것이다. 하지만 새로운 정책은 업데이트 이전에 얻을 수 없다.</p><p>이를 해결하기 위해, 중요도 표본추출(importance sampling)을 통해 식을 변형한다.</p>\[J(\pi')-J(\pi)=\mathbb{E}_{\tau\sim\pi'}[\sum_{t=0}^T\gamma^tA^\pi(s_t,a_t)]\approx\mathbb{E}_{\tau\sim\pi}[\sum_{t\ge0}A^\pi(s_t,a_t)\frac{\pi'(a_t \|s_t)}{\pi(a_t \|s_t)}]=J_\pi^{CPI}(\pi')\]<p>그리고 이를 대리목적(surrogate objective)라고 부른다. 이 새로운 목적함수를 이용한 최적화가 여전히 정책 경사상승을 수행하는지 확인할 필요가 있을 것이다.</p>\[\nabla_\theta J_{\theta_{old}}^{CPI}(\theta)|_{\theta_{old}}=\nabla_\theta\mathbb{E}_{\tau\sim\pi_{\theta_{old}}}[\sum_{t\ge0}A^{\pi_{\theta_{old}}}](s_t,a_t)\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t,s_t)}]|_{\theta_{old}}\] \[=\mathbb{E}_{\tau\sim\pi_{\theta_{old}}}[\sum_{t\ge0}A^{\pi_{\theta_{old}}}(s_t,a_t)\frac{\nabla_\theta\pi_\theta(a_t|s_t)|_{\theta_{old}}}{\pi_\theta(a_t,s_t)|_{\theta_{old}}}]=\mathbb{E}_{\tau\sim\pi_{\theta_{old}}}[\sum_{t\ge0}A^{\pi_{\theta_{old}}}](s_t,a_t)\nabla_\theta\log\pi_\theta(a_t,s_t)|_{\theta_{old}}]\]<p>로그 미분법을 활용한다.</p>\[=\nabla_\theta J(\pi_\theta)|_{\theta_{old}}\Rrightarrow \nabla_\theta J_{\theta_{old}}^{CPI}(\theta)|_{\theta_{old}}=\nabla_\theta J(\pi_\theta)|_{\theta_{old}}\]<p>이를 통해 대리목적을 최적화하는 것이 곧 정책 향상을 최대화하는 것이라는 것을 알게 되었다. 또한, $J_\pi^{CPI}(\pi’)$가 $J(\pi’)-J(\pi)$에 대한 선형근사라는 것도 알 수 있다.</p><p>따라서, $J_\pi^{CPI}(\pi’)$를 활용하기 위해선, $J(\pi’)-J(\pi)\ge0$이 보장되어야 한다. 이는 KL Divergence를 통해 보장할 수 있다.</p>\[|(J(\pi')-J(\pi))-J_\pi^{CPI}(\pi')| \le C\sqrt{\mathbb{E}_t[KL(\pi'(a_t|s_t)||\pi(a_t|s_t)]}\]<p>이 식은 새로운목적함수 $J(\pi’)$와 추정값 $J(\pi) + J_\pi^{CPI}(\pi’)$의 차이에 대한 절댓값 오차를 표현하다. 그리고 이는 KL Divergence를 통해 제한된다.</p><p>이것을 활용하면, 원하는 결과를 유도할 수 있다.</p>\[J(\pi')-J(\pi) \ge J_\pi^{CPI}(\pi')-C\sqrt{\mathbb{E}_t[KL(\pi'(a_t|s_t)||\pi(a_t|s_t)]}\]<p>즉 정책 변화가 의미 있으려면 정책 향상의 추정값이 KL Divergence보다 커야한다. 그렇지 않은 경우 업데이트를 수행하지 않으면 단조 향상이 보장된다.</p><p>최종적으로, 최적화 문제는 다음으로 전환된다.</p>\[argmax_{\pi'}(J_\pi^{CPI}(\pi')-C\sqrt{\mathbb{E}_t[KL(\pi'(a_t|s_t)||\pi(a_t|s_t)]})\]<p>구현 가능성을 위해, 식을 아래와 같이 바꿀 수도 있다.</p>\[max_\theta\mathbb{E}_t[\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t,s_t)}A_t^{\pi_{\theta_{old}}}] \ \ \ st.\ \mathbb{E}_t[KL(\pi'(a_t|s_t)||\pi(a_t|s_t)] \le \delta\]<p>여기서 $\delta$를 신뢰 영역(trust region)이라고 부르고, 이는 튜닝이 필요한 하이퍼파라미터이다.</p><h2 id="73-ppo"><span class="mr-2">7.3 PPO</span><a href="#73-ppo" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>TRPO의 문제점은 계산량이 많고, $\delta$를 선택해야한다는 것이다. PPO는 두 가지 방식으로 이를 해결한다.</p><p>우선, \(r_t(\theta)=\frac{\pi_\theta(a_t \|s_t)}{\pi_{\theta_{old}}(a_t,s_t)}\)를 활용하여, 대리목적을 간단히 표현한다. \(J^{CPI}(\theta)=\mathbb{E}_t[r_t(\theta)A_t]\)</p><h3 id="1-적응-kl-페널티"><span class="mr-2">1. 적응 KL 페널티</span><a href="#1-적응-kl-페널티" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>\[J^{KLPEN}(\theta)=max_\theta\mathbb{E}_t[r_t(\theta)A_t - \beta KL(\pi_\theta(a_t \|s_t)\|\|\pi_{\theta_{old}}(a_t \|s_t)\]<p>이 식은 KL 페널티 대리목적(KL-penalized surrogate objective)이라고 부른다. 이는 PPO에서 $\delta$로 제한조건을 둔 것과 같은 기능을 한다.</p><p>$\beta$가 커지면 두 정책 사이의 거리 유지가 강조되고, 작아지면 허용 오차가 높아진다.</p><p>한편, 이 식에서도 여전히 적절한 $\beta$를 찾는 문제가 발생한다. PPO에서는 $\beta$가 시간에 따라 다르게 적응하는 것을 제안한다.</p><p>즉 $\delta$가 기준보다 작으면 $\beta$를 낮추고, $\delta$가 기준보다 크면 $\beta$를 키운다. 하지만 여전히 기준 $\delta$값 선택 문제를 해결하지 못한다….</p><h3 id="2-대리목적-클리핑"><span class="mr-2">2. 대리목적 클리핑</span><a href="#2-대리목적-클리핑" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>$J^{CLIP}(\theta)=\mathbb{E}_t[min(r_t(\theta)A_t, \ clip(r_t(\theta), 1-\epsilon,1+\epsilon)A_t)]$</p><p>이를 클리핑이 적용된 대리목적(clipped surrogate objective)라고 부른다.</p><p>클리핑을 적용함을 통해, $r_t(\theta)$를 $[1-\epsilon, 1+\epsilon]$의 범위 밖으로 벗어나게 만들 정도로 정책을 변화시킬 이유가 없다.</p><p><a href="../../assets/img/1/Untitled 1.png" class="popup img-link "><img data-src="../../assets/img/1/Untitled 1.png" alt="Untitled" class="lazyload" data-proofer-ignore></a></p><h2 id="74-알고리즘-구현"><span class="mr-2">7.4 알고리즘 구현</span><a href="#74-알고리즘-구현" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/ai/'>AI</a>, <a href='/categories/rl/'>RL</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/book-review/" class="post-tag no-text-decoration" >Book Review</a> <a href="/tags/python/" class="post-tag no-text-decoration" >Python</a> <a href="/tags/ai/" class="post-tag no-text-decoration" >AI</a> <a href="/tags/rl/" class="post-tag no-text-decoration" >RL</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=단단한 심층 강화학습 정리 - Jeong Min's blog&url=https%3A%2F%2Fdgg1dbg.github.io%2Fposts%2F%25EB%258B%25A8%25EB%258B%25A8%25ED%2595%259C-%25EC%258B%25AC%25EC%25B8%25B5-%25EA%25B0%2595%25ED%2599%2594%25ED%2595%2599%25EC%258A%25B5-%25EC%25A0%2595%25EB%25A6%25AC%2F" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=단단한 심층 강화학습 정리 - Jeong Min's blog&u=https%3A%2F%2Fdgg1dbg.github.io%2Fposts%2F%25EB%258B%25A8%25EB%258B%25A8%25ED%2595%259C-%25EC%258B%25AC%25EC%25B8%25B5-%25EA%25B0%2595%25ED%2599%2594%25ED%2595%2599%25EC%258A%25B5-%25EC%25A0%2595%25EB%25A6%25AC%2F" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fdgg1dbg.github.io%2Fposts%2F%25EB%258B%25A8%25EB%258B%25A8%25ED%2595%259C-%25EC%258B%25AC%25EC%25B8%25B5-%25EA%25B0%2595%25ED%2599%2594%25ED%2595%2599%25EC%258A%25B5-%25EC%25A0%2595%25EB%25A6%25AC%2F&text=단단한 심층 강화학습 정리 - Jeong Min's blog" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/WebRTC-%EB%82%B4%EC%9A%A9-%EC%A0%95%EB%A6%AC/">WebRTC 내용 정리</a><li><a href="/posts/Syncthing-Server/">라즈베리 파이로 Syncthing 서버 만들기</a><li><a href="/posts/%EB%8F%84%EB%8D%95%EC%9D%98-%EA%B3%84%EB%B3%B4-%EC%9A%94%EC%95%BD%EA%B3%BC-%ED%95%B4%EC%84%9D/">도덕의 계보 요약과 해석</a><li><a href="/posts/DPDK-%EC%82%AC%EC%9A%A9%EA%B8%B0(1)/">DPDK 사용기(1)</a><li><a href="/posts/html-%EA%B3%B5%EB%B6%80-%EC%A0%95%EB%A6%AC/">html 공부 정리</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/book-review/">Book Review</a> <a class="post-tag" href="/tags/python/">Python</a> <a class="post-tag" href="/tags/frontend/">frontend</a> <a class="post-tag" href="/tags/dpdk/">DPDK</a> <a class="post-tag" href="/tags/hft/">HFT</a> <a class="post-tag" href="/tags/math/">Math</a> <a class="post-tag" href="/tags/network/">Network</a> <a class="post-tag" href="/tags/number-theory/">Number Theory</a> <a class="post-tag" href="/tags/ai/">AI</a> <a class="post-tag" href="/tags/css/">css</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 pl-3 pr-3 pr-xl-4 mt-5"><div id="related-posts" class="mb-2 mb-sm-4"><h3 class="pt-2 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/%ED%8C%8C%EC%9D%B4%EC%8D%AC-%EC%BD%94%EB%94%A9%EC%9D%98-%EA%B8%B0%EC%88%A0(1)-%ED%8C%8C%EC%9D%B4%EC%8D%AC%EB%8B%B5%EA%B2%8C-%EC%83%9D%EA%B0%81%ED%95%98%EA%B8%B0/"><div class="card-body"> <em class="small" data-ts="1662384000" data-df="ll" > Sep 5, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>파이썬 코딩의 기술(1) 파이썬답게 생각하기</h3><div class="text-muted small"><p> 책 파이선 코딩의 기술을 읽고 정리한 내용입니다. 1장. 파이썬 답게 생각하기 1. 사용중인 파이썬의 버전을 알아두라 python3 --version 2. PEP 8 스타일 가이드를 따르라 딕셔너리에서 키와 코론 사이에 공백을 넣지 않고, 콜론과 값 사이에 스페이스를 하나 넣는다. 함수, 변수, 애트리뷰트는 lowercase_u...</p></div></div></a></div><div class="card"> <a href="/posts/%ED%8C%8C%EC%9D%B4%EC%8D%AC-%EC%BD%94%EB%94%A9%EC%9D%98-%EA%B8%B0%EC%88%A0(3)-%ED%95%A8%EC%88%98/"><div class="card-body"> <em class="small" data-ts="1662878220" data-df="ll" > Sep 11, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>파이썬 코딩의 기술(3) 함수</h3><div class="text-muted small"><p> 책 파이선 코딩의 기술을 읽고 정리한 내용입니다. — 3. 함수 19. 함수가 여러 값을 반환하는 경우 절대로 네 값 이상을 언패킹하지 말라 함수의 값을 언패킹할 때 변수가 네 개 이상 나오면 실수하기 쉽다. 20. None을 반환하기보다는 예외를 발생시켜라 특별한 의미를 표시하는 None을 반환하는 함수를 이용하면, 다른 값(0 또는...</p></div></div></a></div><div class="card"> <a href="/posts/%ED%8C%8C%EC%9D%B4%EC%8D%AC-%EC%BD%94%EB%94%A9%EC%9D%98-%EA%B8%B0%EC%88%A0(2)-%EB%A6%AC%EC%8A%A4%ED%8A%B8%EC%99%80-%EB%94%95%EC%85%94%EB%84%88%EB%A6%AC/"><div class="card-body"> <em class="small" data-ts="1662878220" data-df="ll" > Sep 11, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>파이썬 코딩의 기술(2) 리스트와 딕셔너리</h3><div class="text-muted small"><p> 책 파이선 코딩의 기술을 읽고 정리한 내용입니다. — 2. 리스트와 딕셔너리 11. 시퀀스를 슬라이싱하는 방법을 배워라 a = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;, &#39;f&#39;, &#39;g&#39;, &#39;h&#39;] first_twenty = a[:20] last_twenty = a[-20:] # a[20] 또는 a[-20]는 에러 슬라이싱은 리스트의 인...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/js-%EA%B3%B5%EB%B6%80-%EC%A0%95%EB%A6%AC/" class="btn btn-outline-primary" prompt="Older"><p>js 공부 정리</p></a> <a href="/posts/WebRTC-%EB%82%B4%EC%9A%A9-%EC%A0%95%EB%A6%AC/" class="btn btn-outline-primary" prompt="Newer"><p>WebRTC 내용 정리</p></a></div><script type="text/javascript"> $(function () { const origin = "https://giscus.app"; const iframe = "iframe.giscus-frame"; const lightTheme = "light"; const darkTheme = "dark_dimmed"; let initTheme = lightTheme; if ($("html[data-mode=dark]").length > 0 || ($("html[data-mode]").length == 0 && window.matchMedia("(prefers-color-scheme: dark)").matches)) { initTheme = darkTheme; } let giscusAttributes = { "src": "https://giscus.app/client.js", "data-repo": "dgg1dbg/dgg1dbg.github.io", "data-repo-id": "R_kgDOH8CEaQ", "data-category": "General", "data-category-id": "DIC_kwDOH8CEac4CRO2Z", "data-mapping": "pathname", "data-reactions-enabled": "1", "data-emit-metadata": "0", "data-theme": initTheme, "data-input-position": "bottom", "data-lang": "", "crossorigin": "anonymous", "async": "" }; let giscusScript = document.createElement("script"); Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value)); document.getElementById("tail-wrapper").appendChild(giscusScript); addEventListener("message", (event) => { if (event.source === window && event.data && event.data.direction === ModeToggle.ID) { /* global theme mode changed */ const mode = event.data.message; const theme = (mode === ModeToggle.DARK_MODE ? darkTheme : lightTheme); const message = { setConfig: { theme: theme } }; const giscus = document.querySelector(iframe).contentWindow; giscus.postMessage({ giscus: message }, origin); } }); }); </script></div></div></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/book-review/">Book Review</a> <a class="post-tag" href="/tags/python/">Python</a> <a class="post-tag" href="/tags/frontend/">frontend</a> <a class="post-tag" href="/tags/dpdk/">DPDK</a> <a class="post-tag" href="/tags/hft/">HFT</a> <a class="post-tag" href="/tags/math/">Math</a> <a class="post-tag" href="/tags/network/">Network</a> <a class="post-tag" href="/tags/number-theory/">Number Theory</a> <a class="post-tag" href="/tags/ai/">AI</a> <a class="post-tag" href="/tags/css/">css</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><footer><div class="container pl-lg-4 pr-lg-4"><div class="d-flex justify-content-between align-items-center text-muted ml-md-3 mr-md-3"><div class="footer-left"><p class="mb-0"> © 2025 <a href="https://twitter.com/username">Jeong Min Lee</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0">Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></div></footer><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a><div id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-animation="true" data-autohide="false"><div class="toast-header"> <button type="button" class="ml-2 ml-auto close" data-dismiss="toast" aria-label="Close"> <span aria-hidden="true">&times;</span> </button></div><div class="toast-body text-center pt-0"><p class="pl-2 pr-2 mb-3">A new version of content is available.</p><button type="button" class="btn btn-primary" aria-label="Update"> Update </button></div></div><script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No results found.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lazysizes@5.3.2/lazysizes.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/en.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-MT5785FP0G"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-MT5785FP0G'); }); </script>
